<p align="center"><img src="https://zupimages.net/up/20/06/pd6r.png"></p>
<br/>

# SEMAINE 2
# Du 10 février 2020 au 14 février 2020
<br/>

## Lundi 10 février

### Projet Scraping PDF
Passage temporaire sous Python :
* De PDF à TXT
* De PDF à CSV
* De PDF à TSV
* De PDF à HTML

**But :** Tenter de formaliser la data de façon plus accessible pour pouvoir les scrapper et les insérer dans la base de données. Solution non automatisable mais acceptable dans un premier temps pour pouvoir améliorer le modèle existant, quitte, dans un second temps, à trouver une solution plus convenable à long terme.

Extension gardée : TXT / CSV

### Choix de règles afin de préparer le traitement du fichier

### Traitement du document généré avec Python et Jupyter Notebook
Résultats : 
* Normalisation initiée : 
    * Transformation en Dataframe
    * Suppression des espaces et des caractères spéciaux  
    * Suppression des textes inutiles.
* Récupération de certains pays (non insérés dans la base de données) - Solution non-retenue au final
<br/>

## Mardi 11 février

### Projet Scraping PDF
* Repassage sous Java
* Point avec mon référent technique pour voir les obstacles ainsi que les fonctions créées et les règles éditées la veille. 
* Évolution du processus d'importation des données d'un PDF particulier : 
    * Avant mon arrivée, certaines données étaient placées en dur (par exemple la date du document)
    * A mon arrivée, il y a eu 3 évolutions : 
        * :heavy_check_mark: Au tout début (semaine 1), j'ai donné la date en paramètre à mes fonctions et objets mais la date était toujours placée en dur dans la source du code. 
        * :heavy_check_mark: Puis, en début de semaine 2, la date est donnée en paramètre directement dans Postman lors de l'import du PDF 
        * :x: TODO : Récupérer la date sur la première page du document et la donner en paramètre à toute l'application.

### Formation sur Azure ML
<br/>

## Mercredi 12 février

### Projet Scraping PDF
* :heavy_check_mark: Réalisation de la récupération de la date et donnée en paramètre pour toute l'application.
* Push sur la branche blackrock-date
* Ajout de fonctions pour la récupération de tous les pays en gras sur chaque page du PDF
<br/>

## Jeudi 13 février

### Projet Scraping PDF
* :heavy_check_mark: Récupération de tous les pays sous forme de liste
* Règles établies et validées pour associer les sous-parties de fonds (pays avec les shares)
* :x: TODO : Associer les pays aux shares récupérées
<br/>

## Vendredi 14 février

### Projet Scraping PDF
* :heavy_check_mark: Récupération des catégories de produits. Toutes les données nécessaires sont scrappées :muscle:
* :x: TODO : Modifier la base de données pour accueillir les nouvelles features (ajout de champs) 
* :x: TODO : Lancer l'importation des données dans la base de données



---------------------------------

**LA SEMAINE EN BREF :** 
- Expérimentation de procédés alternatifs (conversion dans d'autres formats)
- Récupérer toutes les données souhaitées selon brief s1
- Optimiser le code (factorisation / imbrication de fonctions)
- Faciliter la reprise future du code en rajoutant des commentaires et en prévoyant une Javadoc
- Me familiariser avec Gitlab
- Partager sur Gitlab (merge request)
- Confronter mon code à des déveveloppeurs "aguerris" pour améliorer l'existant / bonnes pratiques de développement collaboratif
- Dynamisation du processus pour qu'il soit applicable à d'autres PDFs à récupérer

---------------------------------
